# CONTENTS
- [2017 Winter](#2017-winter)
    - [CS 519 Deep Learning (->)](#cs-519-deep-learning)
    - [CS 540 Database Managerment (->)](#cs-540-database-managerment)


# 2017 Winter

## CS 540 Database Managerment
- [class note](#class-note)
- [paper review before class (->)](#paper-review)
- [Assignment 4](#assignment-4)
- [Project](#cs-540-project)
- [***Back to CONTENTS***](#contents)


### class note
#### 02/04/2017 Tue


Data dependence
Access path dependence
How you can organize the data
Applications would hard code access paths to data, so would rely on the continued existence of the used access paths.


Levels of abstraction in DBMS
Physical implementation
storage structure indexing, access method

operation on relations: deriving relations
Permutation: interchange the columns of an n-ary relation
Projection: select columns and remove any duplication in the rows
Join: selectively combining tuples in two relations, as a "class" of new relation
Composition
Restriction

what is missing in the set of operators?: order by, group by
Is it minimal?
How is it different from current algebra


redundancy: redundant if can be derived from others; foundation: what operations allowed in derivation
consistency: data snapshot must satisfy some constraints

the advantages of relational model: simplicity, mathematical

does relational model provide data independence?
ordering indep? index indep? acces path indep?
index: some way to organize data, like binary tree.  index does not influence the query


parrallel processing: relations in and out: pipeline: piping the output of one op into the next; partition: N op-clones, each processes 1/N input
Graphical user interfaces: relations fits the spreadsheet(table) metaphor
->table is easier than graph to partition unless it's a complex table

1970: resistance even within IBM; too mathematical, no system
First implementation, 1973

relational data is time consuming.
network/hierarchical models makes a come back.
a great deal of graph data sets: web is a huge network database

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)  
[***Back to CONTENTS***](#contents)

---

### paper review
- [01.12_A Relational Model of Data for Large Shared Data Banks](#0112)
- [01.12_The Universal-Relation Data Model for Logical Inependence](#0112-2)
- [01.19_Operating System Support for Database Management](#0119)
- [01.26_Query Evaluation Techniques for Large Databases](#0126)
- [01.31_Access Path Selection in a Relational Database Management System](#0131)
- [02.07_Granularity of Locks and Degrees of Consistency in a Shared Data Base](#0207)
- [02.09_ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging](#0209)
- [02.28_The Gamma Database Machine Project](#0228)
- [02.28_MapReduce: Simplified Data Processing on Large Clusters](#0228-2)

- 01.31 02.07 02.02 02.09 02.09 02.09


#### 01/12
review_Kaibo Liu_01.12_**A Relational Model of Data for Large Shared Data Banks**

What is the problem discussed in the paper?
This paper discussed data independencies in ralational data model for shared access. There should be minimal or no influence to users at terminal end or in the applications accessing this data when there are internal or external changes of representation of data. And also the problems for data inconsistencies.

Why is it important?
From growth in data types and changes in data representation, independence and inconsistency will become troublesome even in nondeductive systems. The advantages of relational model are it deals with derivability, redundancy, and consistency of relations.

What are the main ideas of the proposed solution for the problem?
All usual set operations can be applicable on relations and result may not be a relation. Permutation, Projection, Join, Composition, and Restriction are some operations specific for relations. The relational model is well explained with its properties such as: Each row is distinct, representing n-tuple in an n-ary relation ‘R’ with no particular order and each column has distinct order and well are defined with a label. Ordering of columns is needed as the order determines the relation in some tables, if the domain names are identical and to deal with time-varying relations. But if the relation is of higher order it is better to have unique domain names and the relations as domain-unordered.

What are the final results and conclusion of the paper?
This paper defined operations on relations and 2 types of redundancy, and applied them to the problem of maintaining the data in a consistent state. Also many questions are raised and left unanswered, this paper had some impact for the time.

Question: With too much too mathematical explanation, how can this paper tell a more specific way in system?

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

#### 01.12-2
review_Kaibo Liu_01.12_**The Universal-Relation Data Model for Logical Inependence**

**What is the problem discussed in the paper?**
This paper discussed a problem of access path independency, which was not compeletely solved in the 1970 paper. A specific example to discribe this problem is, considering a database which has two relations ED(Employee, Department) and DM(Department, Manager), if we're interested in the relationship between employees and managers through departments, we must specify the natural joint of ED and DM relations and project it onto EM. Although this problem may be overvomed by defining a view on EM, that approach ay lead to an unwanted proliferation of views.

**Why is it important?**
A complete access-path independence frees users from concern with the logical structure of the database, and protects users from the errors that creep into queries when complicated access paths must be specified.

**What are the main ideas of the proposed solution for the problem?**
This paper proposed a new model called Universal-Relation Data Model. TThe model is based on an assumption that there is a universal-relation scheme, a set of attributes about which queries may be posed. Further, attributes in this set are assumed to play only one role, and puns are not allowed. Thus, an attribure like name cannot stand for names of employees, customers, suppliers, and managers in the same universal-relation scheme. There is another basic assumprion that for all attribute sets X there's a unique relationship on this set X that the user has in mind. This underlying assumprion is called relationship uniqueness. The connection and query processing consists two steps: binding and evaluation. The two phases are independent.

**What are the final results and conclusion of the paper?**
The universal-relation model gives users a more succinct language for expressing queries, frees them from concern with the databases's logical structure, and protects them from the errors that creep into queries when complicated access paths must be specified. The disadvantage of the universal-relation model is that certain logical naviaation be done automatically by the system may place some subtle constraints on the data structure and may make unusual access paths harder to specify.

**Question**:
What's the efficiency of the approach compared to previous methods?

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

#### 01.19
review_Kaibo Liu_01.19_**Operating System Support for Database Management**

This paper goes over several functions that operating systems of that time already provided, and examines whether the provided services are appropriate for DBMS functions or not. The first of these is the buffer pool management, providing a main memory cache for the file system, which does not work well for databases because the OS does not know which blocks to prefetch into memory, while the DBMS does. Therefore, a DBMS buffer manager has to be created to run in user space, to circumvent the OS's version. In the second part, file system is discussed. The UNIX file system that time takes the data as array. On the other hand database provides an abstraction where user's keys map to records. Constructing database on top of OS filing system is not always efficient due to the following requirement. The file might scattered over the disk and lose the physical contiguity. Second problem is that there are three tree structures: file control block tree, directories tree, and DBMS such as INGRES adds another tree for keyed access via a multilevel directory structure. The authors note that the file hierarchy does nothing for a DBMS, and DB developers must create their own indexes over flat character arrays. The authors then move onto scheduling and process management, and he provides four ways to organize a multiuser database system. Because DBMS manage their own locks to maintain consistency, they must also handle their own scheduling to avoid deadlocks or any other issues. The performance and other cost of replicating this facilities leave quite a bit to be desired, but is the best current option. In this paper Stonebraker mainly discusses how can an operating system be more friendly to database application, and exclaim that the operating system design should be more sensitive to database needs. I think it's an inevitable trade off between generality and specificity.

As the author mentioned at the end of this paper, there are so-called real-time OS which provides minimal facilities which is closed to what the author suggests. And the author hopes that future OS will provide both sets of services in one environment. This is a good idea, but I am a little bit curious whether we need to separate conventional OS with a small efficient OS that provides desired services to DBMS. It would be great if we can achieve both in one environment, however, if it is not possible, what is the disadvantage of developing two different OS separately?


[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---


#### 01.26
review_Kaibo Liu_01.26_**Query Evaluation Techniques for Large Databases**_Sections 5 and 7

In Sections 5 and 7 of this paper,
5.BINARY MATCHING OPERATIONS
  5.1. Nested-Loops Join Algorithms
  5.2 Merge-Join Algorithms
  5.3 Hash Join Algorithms
  5.4 Pointer-Based Joins
  5.5 A Rough Performance Comparison
7.DUALITY OF SORT- AND HASH-BASED QUERY PROCESSING ALGORITHMS

In Section5, the authors go on to describe nested loops, removing duplicates as early as possible, and hashing. I thought the second was a nice improvement, and easy to implement via sorting or hashing. From the graphs, it looks like hybrid hashing is the best way to do aggregation, though of course it is highly dependent on how good the hash function is. Joins are discussed in the next section. There are many techniques, though the authors note that newer ones are sometimes unable to answer the question, "Does this new technique apply to joining three inputs without interrupting data flow between the join operations?" I would like to know more about this question; is it widely considered an important property of join techniques? What kinds of techniques fail this test that have been seriously examined by the database community? Nested loop join, merge-join, and hash-join algorithms are described; we talked about these in class. The heap-filter merge join sounds like a good way to save I/O operations over merge join and be more efficient that nested join. Again, it was emphasized that skew in the hash function messes things up for hash join.

The paper reviews different operational aspects of a DBMS's query execution engine. The most prominent aspects of the sections we reviewed are the treatment of sorting and hashing algorithms. Sorting - the algorithm described for sorting is essentially quicksort of datasets that fit in memory and merge for larger datasets. Sorting-based aggregation outperforms nested-loops and aggregating as early as possible is useful. Merge-join, the sorting based join algorithm is widely used since it's rough cost can be estimated and it performs well enough. Can also be used to implement division between tables. Hashing - hybrid hashing writes only necessary data to disk saving I/O cost. Aggregation based on hybrid hash performs better than sorting-based algorithm with early aggregation. Although there is an overhead in computing the hash value, the search space is made smaller and so it outperforms merge-join. Combined with bit map, the hash method is efficient for processing division operation. The paper also briefly covers Indexing. The paper compares various index structures according to their support for ordering and sorted scans and their support for point data and range data.

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---


#### 01.31
review_Kaibo Liu_01.31_**Access Path Selection in a Relational Database Management System**

https://blog.acolyer.org/2016/01/04/access-path-selection/
This paper introduces the idea of a query optimizer, built as part of System R, that plans the most efficient way to retrieve the data requested by a SQL query. As well as giving insight into how a query optimizer may be constructed, the paper also quietly introduced the important result that a declarative query language can be supported with no loss of performance compared to the more common procedural query language approaches of the day. Declarative query language also relieves programmer of the burden to choose an access plan. Difference between good and bad access plans can be several orders of magnitude. And there are three problems in choosing a good plan: what is the plan space? How to estimate cost? How to choose a plan? To find the optimal plan for join operations, a tree of possible solutions is constructed. There is a worked example in the paper of joining Employee, Job title, and Department tables that helps to make this process clearer. The cost of a join is computed based on the costs of scans on each of the relations (using the cost estimate formulas we saw earlier) and the cardinalities.

**Question**:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

#### 02.07
review_Kaibo Liu_02.07_**Granularity of Locks and Degrees of Consistency in a Shared Data Base**

This paper is divided in two sections: granularity of locks, and degrees of consistency. Each section answers questions on how lock choice in a database affects throughput and consistency.

In the granularity section, the choice of lockable units is discussed. A lockable unit represents a section of logical data that is atomically locked during a transaction. Locking smaller units such as individual records improves concurrency for  "simple" transactions that access a small number of records. On the other hand, locking at a record level can reduce throughput for "complex" transactions that require access to many records — the overhead of acquiring and releasing locks overwhelms the computation. It follows that having different sizes of lockable units in the same system is required to handle multiple use cases. Two types of locks are presented: X or exclusive locks, and S or shared locks.

The second section defines four different modes of labelled degrees 0 through 3.
· A degree 0 transaction is the least restrictive type. It promises to not over-write data from other transactions. It requires having any transaction take a lock on any data it writes.
- Degree 1(Read Uncommitted) consistency keeps the promise of Degree 1 (not to overwrite data) and also agrees not to commit any writes until the end of the transaction. In this case, you may require a longer lock that is held until the end of the transaction.
- Degree 2 (Read Committed) consistency further restricts a transaction to only read values that have been committed (to contrast, a degree 1 transaction may read dirty values). In addition to acquiring locks for all data written during the transaction, a degree 2 transaction acquires locks for all data read during the transaction.
- Degree 3 (Repeatable Read) consistency completely isolates a transaction from each other. It acquires long-lived locks on both data read and data written. Degree 3 provides the highest level of isolation described in the paper.

**Question**:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---


#### 02.09
review_Kaibo Liu_02.09_**ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging**_<=Section 6.3

New algorithms for database recovery and rollbacks are described. The paper assumes that the database uses write-ahead logging (WAL), but it describes in fine detail how the various activities during the update, rollback, and recovery phases are to act so as to maximize concurrency and minimize both overhead and time. In their introduction, the authors also provide an excellent description of the current state of the art of logging, failures, and recovery methods. The paper is broken into 12 sections and has an extensive bibliography (101 citations). The sections are an introduction, goals, an overview of ARIES, a description of the major data structures, a discussion of the actions that are part of normal processing (including transaction failure), a description of restart processing (after system failure), a description of the impact of checkpoints during restart, the methods necessary for media recovery, top actions (independent transactions kicked off by running transactions such as file extension), recovery paradigms (mostly problems caused by them), properties of other WAL-based methods (including references to several commercial implementations), and a summary of the attributes of ARIES. As the proposed solution, the fundamental idea of database recovery is to log logical changes to the database to durable storage before applying those changes to the actual database. If this protocol is followed, then any failures can be recovered by using the change log itself. This simple idea is used throughout the paper to illustrate the recovery algorithms for transaction failure, crashes, and storage media failure.
As part of the ARIES protocol, each log is assigned a log sequence number (LSN) uniquely identifying the log record. Further, each page in the database records the LSN that modified the page. ARIES also tracks any in-flight transactions to be able to fully restore the database to the point-in-time of the crash before doing full recovery.
To perform recovery, ARIES uses the log and the transaction table during three passes: analysis, redo, and undo. During the analysis pass, the log is scanned to extract the dirty pages and in-flight transactions from the point of failure, determining the starting point where redo is required, and the in-flight transactions that must be rolled back. During the redo pass, the database is restored to the state it was at the time of failure. Finally, during undo pass, any in-flight transactions that failed have their changes rolled back. Combining these three passes restores the database to a consistent state.

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

#### 02.28
review_Kaibo Liu_02.28_**The Gamma Database Machine Project**

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

#### 02.28-2
review_Kaibo Liu_02.28_**MapReduce: Simplified Data Processing on Large Clusters**

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

review_Kaibo Liu_02.23_Scalable SQL and NoSQL Data Stores

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

review_Kaibo Liu_02.23_Crash Recovery in a Distributed Data Storage System

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

review_Kaibo Liu_02.28_Bigtable: A Distributed Storage System for Structured Data

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

review_Kaibo Liu_02.28_Dynamo: Amazon's Highly Available Key-Value Store_section4.4+4.5

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---


review_Kaibo Liu_03.02_Answering Queries Using Views: A Survey

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

review_Kaibo Liu_03.09_The PageRank Citation Ranking: Bringing Order to the Web

Question:

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)

---

review_Kaibo Liu_03.09_Efficient IR-Style Keyword Search over Relational Databases

Question:


https://web.eecs.umich.edu/~mozafari/fall2015/eecs584/reviews/summaries/

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)  
[***Back to CONTENTS***](#contents)

---


### Assignment 4
```
gantt
dateFormat DD
section T1
R(X): 01, 2d
R(Y): 09, 2d
section T2
R(Y): 03, 2d
R(X): 07, 2d
section T3
W(X): 05, 2d
```

```
gantt
dateFormat DD
section T1
R(X): 01, 1d
R(Y): 02, 1d
W(X): 03, 1d
W(X): 06, 1d
section T2
R(Y): 04, 1d
R(Y): 07, 1d
section T3
W(Y): 05, 1d
```

```
gantt
dateFormat DD
section T1
W(X): 01, 1d
W(X): 03, 1d
Commit: 05, 1d
section T2
R(X): 02, 1d
Commit: 04, 1d
```

```
gantt
dateFormat DD
section T1
R(X): 01, 1d
W(X): 03, 1d
Commit: 05, 1d
section T2
W(X): 02, 1d
Commit: 06, 1d
section T3
R(X): 04, 1d
Commit: 07, 1d
```

```
graph TD
DB-->R1
DB-->R2
R1-->t1
R1-->t2
R2-->t3
R2-->t4
R2-->t5
```

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)
[***Back to CONTENTS***](#contents)

---

### Assignment 5

1. log in to server `ssh liukaib@hadoop-master.engr.oregonstate.edu`.
1. add the following to ~/.cshrc
```csh
setenv JAVA_HOME "/usr/lib/jvm/java-1.8.0"
setenv PATH "$JAVA_HOME/bin:$PATH"
setenv CLASSPATH ".:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar"
setenv HADOOP_HOME /opt/hadoop/hadoop
setenv HADOOP_COMMON_HOME $HADOOP_HOME
setenv HADOOP_HDFS_HOME $HADOOP_HOME
setenv HADOOP_MAPRED_HOME $HADOOP_HOME
setenv HADOOP_YARN_HOME $HADOOP_HOME
setenv HADOOP_OPTS "-Djava.library.path=$HADOOP_HOME/lib/native"
setenv HADOOP_COMMON_LIB_NATIVE_DIR $HADOOP_HOME/lib/native
setenv PATH $HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH
setenv HADOOP_CLASSPATH "${JAVA_HOME}/lib/tools.jar"
```
### don't forget `source /.cshrc`
But, the shell in server is bash (use `echo $0` to display current shell). When I use `source /.cshrc`, there is a syntax error with `set path` even I switch shell to csh with `exec /bin/csh`. I didn't know how to deal with it. Then I put all the variables into `~/.bashrc`. However, `setenv` is not supported in bash so I can't source it in bash.
The final solution is, to change `setenv` into `export $var` as bash format:
In `~/.bashrc` (or the first of `~/.bash_profile`, `~/.bash_login`, and `~/.profile` that exists), source this script (saved as something like `~/sourcecsh`) using `. ~/sourcecsh`:
```zsh
#!/bin/bash
# This should be sourced rather than executed
while read cmd var val
do
    if [[ $cmd == "setenv" ]]
    then
        eval "export $var=$val"
    fi
done < ~/.cshrc
```
1. unzip the assignment package downloaded in home path with `wget [url]`

[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)
[***Back to CONTENTS***](#contents)

---

### CS 540 Project
#### Architecture


```
sequenceDiagram
    opt input
        Note left of User: user has something
        User->>System: Keywords
        Note right of System: SELECT * FROM DB WHERE keywords
        System-->>System: Predict in network
        System-->>User: Return (top k enterties)
        User->>System: Choose/Click from k enteries
        System-->>System: Train with new data
    end

    opt no input
        Note left of User: user has no clue
        User->>System: Void keyword
        Note right of System: project database
        System-->>System: Predict in network
        System-->>User: Return (top k enterties)
    end
```
[^_^]:#(
sequenceDiagram
Note left of User: user has something
User->>System: Keywords
Note right of System: SELECT keywords in database -> project
System-->>System: Predict in network
System-->>User: Return (top k enterties)
User->>System: Choose/Click from k enteries
System-->>System: Train with new data
Note left of User: user has no clue
User->>System: Void keyword
    Note right of System: project database
System-->>System: Predict in network
System-->>User: Return (top k enterties)
)


[***Back*** to subcontents ***CS 540***](#cs-540-database-managerment)
[***Back to CONTENTS***](#contents)

---

## CS 519 Deep Learning
- [Assignment 2](#02042017-sat)
- [Project](#cs-519-project)
- [***Back to CONTENTS***](#contents)



#### 02/04/2017 Sat
Assignment 2 of CS519
cPickle.load(open("cifar_2class_py2.p","rb"))取出的dict数组有4个成员
`dict["train_data"], dict["train_labels"], dict["test_data"], dict["test_labels]`。
通过`print type(variable_name)`发现，每一个成员都是一个numpy的二维数组，其中_data每行为一幅图，行数为sample数，_labels为对应图的标签airplane-0/ship-1。可以reshape后看图片。
运行发现没有matplotlib，于是安装。


[***Back*** to subcontents ***CS 519***](#cs-519-deep-learning)


#### 02/05/2017 Sun
Assignment 2 of CS519
无数次试错输出CIFAR-10图形。通过查阅得知data的每行数据（一幅图）中，3072个数据的前1024个数据为该32××32图像的red通道，而随后的1024个数据为green通道，最后的1024个数据为blue通道。
所以reshape时要先分成3份，再32份（行h）和32份（列w），再然后转置.T或者.transpose(1,2,0)，表示分成的3份要放到最后一个轴，形成(32X32X3)，经过这样变换，我们可以很方便的获取想要的图像和图像中对应像素的值。
输出时，用`plt(import matplotlib.pyplot as plt)`的`imshow`，但是图像很模糊有马赛克。虽然分辨率本身很低但是和网上CIFAR的样图还是有差距，于是查到`plt.imshow`有个参数`interpolation`，置`'nearest'`和`'none'`时都是不差值直接按原像素显示，`'spline36'`、`'sinc'`等都会根据不同差值算法来模糊。最后选了个还不错的用来visualization。


[***Back*** to subcontents ***CS 519***](#cs-519-deep-learning)


#### 02/06/2017 Mon
Assignment 2 of CS519
Batch Training: The gradients calculated at each training example are added together to determine the change in the weights and biases. For a discussion of batch training with the backpropagation algorithm see page 12-7 of [HDB96].
A compromise between computing the true gradient and the gradient at a single example, is to compute the gradient against more than one training example (called a "mini-batch") at each step. This can perform significantly better than true stochastic gradient descent because `the code can make use of vectorization libraries rather than computing each step separately`. It may also `result in smoother convergence, as the gradient computed at each step uses more training examples`.
b=10 #mini-batch size, get b examples, x(i),y(i)...x(i+b-1),y(i+b-1)
wj=wj-a\sigma△wj/b

Summing the gradients due to individual samples you get a much smoother gradient. The larger the batch the smoother the resulting gradient used in updating the weight.
Dividing the sum by the batch size and taking the average gradient has the effect of:
1. The magnitude of the weight does not grow out of proportion. Adding L2 regularization to the weight update penalizes large weight values. This often leads to improved generalization performance. Taking the average, especially if the gradients happen to point in the same direction, keep the weights from getting too large.
1. The magnitude of the gradient is independent of the batch size. This allows comparison of weights from other experiments using different batch sizes.
1. Countering the effect of the batch size with the learning rate can be numerically equivalent but you end up with a learning rate that is implementation specific. It makes it difficult to communicate your results and experimental setup if people cannot relate to the scale of parameters you're using and they'll have trouble reproducing your experiment.

Averaging enables clearer comparability and keeping gradient magnitudes independent of batch size. Choosing a batch size is sometimes constrained by the computational resources you have and you want to mitigate the effect of this when evaluating your model.

[Neural Networks: weight change momentum and weight decay](http://stats.stackexchange.com/questions/70101/neural-networks-weight-change-momentum-and-weight-decay)

- one **epoch** = one forward pass and one backward pass of all the training examples
- **batch size** = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.
- number of **iterations** = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).

1 epoch=ierations*batch size


[***Back*** to subcontents ***CS 519***](#cs-519-deep-learning)



#### 02/10/2017 Tue
Assignment 2 of CS519
Architecture
```
graph LR
id1((x:d*batch))-->id2((z1:m*batch))
id2((z1:m*batch))-->id3((z2:m*batch))
id3((z2:m*batch))-->id4((z3:1*batch))
id4((z3:1*batch))-->id5((p,e:1*batch))
```
```
graph LR
id1(W:d*m)-->id0((hidden layer))
id2(b:m*1)-->id0((hidden layer))
id0((hidden layer))-->id3(w:m*1)
id0((hidden layer))-->id4(c:1*1)

```

[***Back*** to subcontents ***CS 519***](#cs-519-deep-learning)

[***Back to CONTENTS***](#contents)

---

### Assignment3
#### 02/25/2017 Sat
`vim ~/.theanorc`, add:
```zsh
[global]
floatX = float32
device = gpu0
```
Above path is on pelican.eecs.oregonstate.edu, the CUDA root is `/usr/local/eecsapps/cuda/cuda-7.5.18`.
For TitanX server, CUDA root is `/usr/local/cuda-8.0/`


`vim ~/.bashrc`, add CUDA path:
```zsh
export PATH=$PATH:/usr/local/eecsapps/cuda/cuda-7.5.18/bin
export LD_LIBRARY_PATH=/usr/local/eecsapps/cuda/cuda-7.5.18/lib64
```

`vim ~/.keras/keras.json` is to switch backend, commend out `"image_dim_ordering": "tf",`

In TitanX server, there are encoding issues loading data, no matter python2 or python3. I add CUDA path (found with `which nvcc`) to .bashrc, then default python2 is able to use GPU in keras as well.(seems useless because python2 can use GPU without this path as well)
Use theano as backend, display `Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5110)`, error is `ValueError: negative dimensions are not allowed`
Use tensorflow as backend, display `opened CUDA library libcublas.so.8.0 locally`, error is `ValueError: Negative dimension size caused by subtracting 3 from 1 for 'Conv2D_1' (op: 'Conv2D') with input shapes: [?,1,16,32], [3,3,32,32].`

However, in pelican server, everything seems correct, but runtime for an epoch is ~500s, neither >2000s nor <50s. Wired...

# FUCK!! I didn't use `source` after modifying ~/.bashrc

After correct configuration, runtime for an epoch is 15s
#### CS 519 Project


[***Back to CONTENTS***](#contents)

---
